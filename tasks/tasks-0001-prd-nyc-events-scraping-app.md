## Relevant Files

- `src/scrapers/kings.js` - Kings Theatre scraper using Stagehand
- `src/scrapers/prospect_park.js` - Prospect Park events scraper using Stagehand  
- `src/scrapers/msg_calendar.js` - MSG Calendar scraper using Stagehand
- `database.py` - Database connection and operations
- `models.py` - SQLAlchemy models for scrape_runs, raw_events, and clean_events
- `src/import_scraped_data.py` - Data import script for raw_events table
- `src/test_scrapers.py` - Scraper testing and reporting framework
- `src/clean_events.py` - Data cleaning pipeline (within source)
- `src/deduplicate_across_sources.py` - Cross-source deduplication
- `app.py` - Flask web application for browsing events
- `templates/base.html` - Base HTML template with styling
- `templates/index.html` - Home page with event listings and filters
- `templates/event_detail.html` - Individual event detail page
- `templates/error.html` - Error page template
- `static/css/style.css` - Main stylesheet with responsive design
- `static/js/main.js` - JavaScript for enhanced interactivity
- `src/extract_event_times.py` - Reusable event time extractor utility
- `integrate_time_extraction.py` - Integration script for Kings Theatre workflow
- `test_time_extractor.py` - Test script for time extractor
- `docs/event-time-extractor.md` - Documentation for time extractor utility
- `requirements.txt` - Python dependencies
- `config.py` - Configuration file (database URLs, API keys)
- `README.md` - Setup and usage instructions

### Notes

- Using Stagehand with Browserbase for web scraping automation
- PostgreSQL database with snapshot-based raw_events table design
- ScrapeRun tracking for audit trail and run comparison
- Comprehensive testing and reporting for scraper reliability
- Two-stage data cleaning: within-source then cross-source deduplication
- Flask web interface with responsive design
- Vercel deployment with GitHub Actions for automated scraping

## Tasks

- [x] 1.0 Basic Project Setup
  - [x] 1.1 Create simple requirements.txt with minimal dependencies
  - [x] 1.2 Set up basic project structure (scrapers/, templates/, static/)
  - [x] 1.3 Create config.py for local DB and Browserbase API key
  - [x] 1.4 Set up basic logging
  - [x] 1.5 Add code linter (flake8 or black)
- [x] 2.0 Local Database Setup
  - [x] 2.1 Install PostgreSQL locally (or use SQLite for even simpler start)
  - [x] 2.2 Create simple SQLAlchemy models (raw_events, clean_events)
  - [x] 2.3 Set up database connection and create tables
  - [x] 2.4 Test database connection
- [x] 3.0 Browserbase Integration
  - [x] 3.1 Install and configure Browserbase Python SDK
  - [x] 3.2 Create simple Browserbase client wrapper
  - [x] 3.3 Test basic browser session creation
- [x] 4.0 Simple Scrapers (3 scrapers)
  - [x] 4.1 Create Kings Theatre scraper (https://www.kingstheatre.com/)
  - [x] 4.2 Create Prospect Park events scraper (https://www.prospectpark.org/events/)
  - [x] 4.3 Create MSG Calendar scraper (https://www.msg.com/calendar?venues=KovZpZA7AAEA)
  - [x] 4.4 Test each scraper individually
- [x] 5.0 Scraper Testing & Reporting
  - [x] 5.1 Create scraper test framework (src/test_scrapers.py)
  - [x] 5.2 Implement run comparison logic (current vs previous run)
  - [x] 5.3 Add removed events detection and reporting
  - [x] 5.4 Generate JSON reports and console summaries
  - [x] 5.5 Update all scrapers to call test script after import
  - [x] 5.6 Add start time collection validation (detect midnight times issue)
- [x] 6.0 Data Cleaning Pipeline (Within Source)
  - [x] 6.1 Create data cleaning script (src/clean_events.py)
  - [x] 6.1.1 Fix architecture: READ ONLY from raw_events, WRITE ONLY to clean_events
  - [x] 6.2 Implement latest run detection per source
  - [x] 6.2.1 Add step to clear existing clean events before processing latest run
  - [x] 6.3 Add within-source deduplication logic
  - [x] 6.4 Standardize data formatting (title case, venue normalization)
  - [x] 6.5 Add quality control validation (ensure start_time present)
  - [x] 6.6 Create CLI for cleaning individual sources or all sources
- [x] 7.0 Web Interface
  - [x] 7.1 Create Flask application (app.py) with routes
  - [x] 7.2 Build HTML templates (base, index, event_detail)
  - [x] 7.3 Add static assets (CSS, JavaScript)
  - [x] 7.6 Create JSON API endpoint for events
- [ ] 8.0 Ensure Consistency & Debugging
  - [x] 8.1 Debug Kings Theater - COMPLETED: Created reusable event time extractor using BeautifulSoup + requests. Successfully extracts times from individual event pages without browser context issues.
  - [x] 8.2 Debug MSG event time start issues - COMPLETED: Fixed MSG scraper extraction instruction to properly extract times from calendar page. Updated import script to handle "ET" timezone suffix. MSG events now extract times correctly (96/96 events with times in test).
  - [ ] 8.3 Ensure the data cleaning script is running after each scraping session to test for any regressions on things like start time (or lots of events missing between last and current run)
- [ ] 9.0 GitHub Actions Automation & Scheduled Scraping
  - [ ] 9.1 Create GitHub Actions workflow file (.github/workflows/scrape.yml)
  - [ ] 9.2 Configure cron schedule for weekly scraper runs
  - [ ] 9.3 Set up GitHub secrets for BROWSERBASE_API_KEY and DATABASE_URL
  - [ ] 9.4 Configure Node.js environment and dependencies
  - [ ] 9.5 Configure Python environment and dependencies
  - [ ] 9.6 Add workflow to run all scrapers sequentially
  - [ ] 9.7 Add automatic data cleaning after scraper runs
  - [ ] 9.8 Configure workflow notifications for success/failure
  - [ ] 9.9 Test workflow with manual trigger
  - [ ] 9.10 Test complete automation pipeline end-to-end
- [ ] 10.0 Neon Database Migration
  - [ ] 10.1 Set up Neon PostgreSQL database
  - [ ] 10.2 Update config.py for environment-based database URLs
  - [ ] 10.3 Create migration script (src/migrate_to_neon.py)
  - [ ] 10.4 Test web app with Neon database
- [ ] 11.0 Vercel Deployment
  - [ ] 11.1 Create Vercel configuration (vercel.json)
  - [ ] 11.2 Create requirements-vercel.txt (web app dependencies only)
  - [ ] 11.3 Configure environment variables in Vercel
  - [ ] 11.4 Deploy web app to Vercel
  - [ ] 11.5 Test complete pipeline with automated deployment
- [ ] 12.0 Cross-Source Deduplication
  - [ ] 12.1 Create deduplication script (src/deduplicate_across_sources.py)
  - [ ] 12.2 Implement fuzzy matching algorithm (85% title similarity)
  - [ ] 12.3 Add venue normalization for matching
  - [ ] 12.4 Create merge logic for duplicate events
  - [ ] 12.5 Generate JSON report of duplicates and merge decisions
  - [ ] 12.6 Add python-Levenshtein dependency
- [ ] 13.0 Enhanced Web Interface Features
  - [x] 13.1 Implement filtering by date range, venue, category - PARTIAL: Venue filtering implemented with display_venue. Date range and category filtering pending.
  - [x] 13.2 Add search functionality and pagination - COMPLETED: Pagination implemented. Search functionality pending.
- [ ] 14.0 Further Extensions
  - [ ] 14.1 Adding day of week filtering
