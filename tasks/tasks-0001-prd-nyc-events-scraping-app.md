## Relevant Files

- `src/scrapers/kings.js` - Kings Theatre scraper using Stagehand
- `src/scrapers/prospect_park.js` - Prospect Park events scraper using Stagehand  
- `src/scrapers/msg_calendar.js` - MSG Calendar scraper using Stagehand
- `database.py` - Database connection and operations
- `models.py` - SQLAlchemy models for scrape_runs, raw_events, and clean_events
- `src/import_scraped_data.py` - Data import script for raw_events table
- `src/test_scrapers.py` - Scraper testing and reporting framework
- `src/clean_events.py` - Data cleaning pipeline (within source)
- `src/deduplicate_across_sources.py` - Cross-source deduplication
- `app.py` - Flask web application for browsing events
- `templates/base.html` - Base HTML template with styling
- `templates/index.html` - Home page with event listings and filters
- `templates/event_detail.html` - Individual event detail page
- `templates/error.html` - Error page template
- `static/css/style.css` - Main stylesheet with responsive design
- `static/js/main.js` - JavaScript for enhanced interactivity
- `requirements.txt` - Python dependencies
- `config.py` - Configuration file (database URLs, API keys)
- `README.md` - Setup and usage instructions

### Notes

- Using Stagehand with Browserbase for web scraping automation
- PostgreSQL database with snapshot-based raw_events table design
- ScrapeRun tracking for audit trail and run comparison
- Comprehensive testing and reporting for scraper reliability
- Two-stage data cleaning: within-source then cross-source deduplication
- Flask web interface with responsive design
- Vercel deployment with GitHub Actions for automated scraping

## Tasks

- [x] 1.0 Basic Project Setup
  - [x] 1.1 Create simple requirements.txt with minimal dependencies
  - [x] 1.2 Set up basic project structure (scrapers/, templates/, static/)
  - [x] 1.3 Create config.py for local DB and Browserbase API key
  - [x] 1.4 Set up basic logging
  - [x] 1.5 Add code linter (flake8 or black)
- [x] 2.0 Local Database Setup
  - [x] 2.1 Install PostgreSQL locally (or use SQLite for even simpler start)
  - [x] 2.2 Create simple SQLAlchemy models (raw_events, clean_events)
  - [x] 2.3 Set up database connection and create tables
  - [x] 2.4 Test database connection
- [x] 3.0 Browserbase Integration
  - [x] 3.1 Install and configure Browserbase Python SDK
  - [x] 3.2 Create simple Browserbase client wrapper
  - [x] 3.3 Test basic browser session creation
- [x] 4.0 Simple Scrapers (3 scrapers)
  - [x] 4.1 Create Kings Theatre scraper (https://www.kingstheatre.com/)
  - [x] 4.2 Create Prospect Park events scraper (https://www.prospectpark.org/events/)
  - [x] 4.3 Create MSG Calendar scraper (https://www.msg.com/calendar?venues=KovZpZA7AAEA)
  - [x] 4.4 Test each scraper individually
- [x] 5.0 Scraper Testing & Reporting
  - [x] 5.1 Create scraper test framework (src/test_scrapers.py)
  - [x] 5.2 Implement run comparison logic (current vs previous run)
  - [x] 5.3 Add removed events detection and reporting
  - [x] 5.4 Generate JSON reports and console summaries
  - [x] 5.5 Update all scrapers to call test script after import
  - [x] 5.6 Add start time collection validation (detect midnight times issue)
- [x] 6.0 Data Cleaning Pipeline (Within Source)
  - [x] 6.1 Create data cleaning script (src/clean_events.py)
  - [x] 6.1.1 Fix architecture: READ ONLY from raw_events, WRITE ONLY to clean_events
  - [x] 6.2 Implement latest run detection per source
  - [x] 6.2.1 Add step to clear existing clean events before processing latest run
  - [x] 6.3 Add within-source deduplication logic
  - [x] 6.4 Standardize data formatting (title case, venue normalization)
  - [x] 6.5 Add quality control validation (ensure start_time present)
  - [x] 6.6 Create CLI for cleaning individual sources or all sources
- [ ] 7.0 Web Interface
  - [x] 7.1 Create Flask application (app.py) with routes
  - [x] 7.2 Build HTML templates (base, index, event_detail)
  - [x] 7.3 Add static assets (CSS, JavaScript)
  - [x] 7.6 Create JSON API endpoint for events
- [ ] 8.0 Neon Database Migration
  - [ ] 8.1 Set up Neon PostgreSQL database
  - [ ] 8.2 Update config.py for environment-based database URLs
  - [ ] 8.3 Create migration script (src/migrate_to_neon.py)
  - [ ] 8.4 Test web app with Neon database
- [ ] 9.0 Vercel Deployment
  - [ ] 9.1 Create Vercel configuration (vercel.json)
  - [ ] 9.2 Create requirements-vercel.txt (web app dependencies only)
  - [ ] 9.3 Configure environment variables in Vercel
  - [ ] 9.4 Deploy web app to Vercel
  - [ ] 9.5 Set up GitHub Actions workflow for weekly scraper runs
  - [ ] 9.6 Test complete pipeline with automated deployment
- [ ] 10.0 Cross-Source Deduplication
  - [ ] 10.1 Create deduplication script (src/deduplicate_across_sources.py)
  - [ ] 10.2 Implement fuzzy matching algorithm (85% title similarity)
  - [ ] 10.3 Add venue normalization for matching
  - [ ] 10.4 Create merge logic for duplicate events
  - [ ] 10.5 Generate JSON report of duplicates and merge decisions
  - [ ] 10.6 Add python-Levenshtein dependency
- [ ] 11.0 Enhanced Web Interface Features
  - [ ] 11.1 Implement filtering by date range, venue, category
  - [ ] 11.2 Add search functionality and pagination
- [ ] 12.0 Further Extensions
  - [ ] 12.1 Adding day of week filtering
